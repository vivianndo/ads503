---
title: "Diabetes Readmission Prediction"
author: "Vivian Do, Bethany Wang"
date: "2023-06-24"
output: html_document
---

### 1. Initial Data Preparation<br><br>

#### <b>1.1 Import and check the Data</b><br><br>

```{r}
#Read in data, replace "?" with NA values 
df <- read.csv(file = "diabetic_data.csv", na.strings=c("?"))
dim(df)
str(df)
```
* There are 101766 data records with 50 features


#### <br><b>1.2 Filter out Irrelevant Observations</b><br><br>

* All patient encounters associated with a discharge to hospice or death will be removed, as the chance of readmission is low-impossible. 

```{r}
dischargedRemoved <- c(11,13,14,19,20,21) 
df <- subset(df, !(discharge_disposition_id %in% dischargedRemoved))
```

* encounter_id and patient_nbr are identifiers, not useful features. Payer_code is irrelevant feature too. They will be removed.


```{r}
df <- subset(df, select = -c(encounter_id, patient_nbr, payer_code))
dim(df)
```


#### <br><b>1.3 Filter out degenerated features</b><br><br>

```{r message=FALSE, warning=FALSE}

library(caret)

# Use nearZeroVar function to filter out low variance features
degenerateCols <- nearZeroVar(df)
length(degenerateCols)

degenerateColNames <- colnames(df[degenerateCols])
degenerateColNum <- length(degenerateColNames)

cat("These are ", degenerateColNum, " degenerated predictors:\n", degenerateColNames, "\n\n")

df<- df[, -degenerateCols]
dim(df)
colnames(df)
```

* 18 degenerated predictors that show near-zero variance have been removed. 29 columns are left in the dataset.

#### <br><b>1.4 Filter out categorical levels indicating nulls</b><br><br>

'discharge_disposition_id' (discharge ID), 'admission_type_id' (admission type ID), and 'admission_source_id' (admission source ID) are categorical variables with levels identified by their ID number. For example, 'admission_type_id' 1 refers to 'Emergency'. 

For admission source and type, the following ID levels associated with null values will be replaced as NA:

* Admission source ID 9 (Not Available), 17 (NULL), and 20 (Not Mapped)
* Admission type ID 6 (NULL) and 8 (Not Mapped)
```{r}
admissionsourceRemoved <- c(9,17,20)
df$admission_source_id[df$admission_source_id %in% admissionsourceRemoved] <- NA

admissiontypeRemoved <- c(6,8)
df$admission_type_id[df$admission_type_id %in% admissiontypeRemoved] <- NA
table(df$admission_source_id)
sum(is.na(df$admission_source_id))
```

#### <br><b>1.5 Handle missing values</b><br><br>

* Check missing values

```{r}
# Check null values
#Show null counts
null_counts <- sort(colSums(is.na(df)), decreasing=TRUE)
head(null_counts,10)

#Show proportion of null counts
nullProp <- sort(round(colMeans(is.na(df)),3), decreasing=TRUE)
head(nullProp,10)
```


* The following predictors contain a significant number of null values: 'weight' (96.9%), medical_specialty (48.9%). These two predictors will be removed.

* The following predictors contain less than 2% null values: 'race' (2.2%), 'diag_3' (1.4%), 'diag_2' (0.4%), and 'diag_1' (0.02%). Because the null percentages are very low, we will simply remove the rows with null values.

```{r}
# remove weight and medical_specialty
df<- subset(df, select = -c(weight, medical_specialty))

# Remove other null values
df <- na.omit(df)

null_counts <- sort(colSums(is.na(df)), decreasing=TRUE)
head(null_counts,10)

dim(df)
```

#### <br><b>1.6 Feature Creation</b><br><br>

* Explore primary diagnoses ('diag_1') and secondary diagnoses ('diag_2', 'diag_3')

```{r message=FALSE, warning=FALSE, out.height="300"}
# Convert to numeric
df$diag_1 <- as.numeric(df$diag_1)
df$diag_2 <- as.numeric(df$diag_2)
df$diag_3 <- as.numeric(df$diag_3)

# Show distributions of diagnoses 
par(mfrow=c(1,3))
hist(df$diag_1)
hist(df$diag_2)
hist(df$diag_3)
#summary(df$diag_1)
```

* Extract primary/secondary diagnoses

```{r message=FALSE, warning=FALSE}
library(dplyr)

df <- df %>%
  mutate(primary_diagnosis = case_when(
    substr(diag_1, 1, 3) == "250" ~ "Diabetes",
    #between(as.numeric(diag_1), 001, 139) ~ "Infectious and Parasitic Diseases",
    between(as.numeric(diag_1), 140, 239) ~ "Neoplasms",
    between(as.numeric(diag_1), 320, 459) ~ "Circulatory",
    between(as.numeric(diag_1), 460, 519) ~ "Respiratory",
    between(as.numeric(diag_1), 520, 579) ~ "Digestive",
    between(as.numeric(diag_1), 580, 629) ~ "Genitourinary",
    #between(as.numeric(diag_1), 680, 709) ~ "Skin/Subcutaneous Tissue",
    between(as.numeric(diag_1), 710, 739) ~ "Musculoskeletal",
    between(as.numeric(diag_1), 760, 779) ~ "Perinatal",
    between(as.numeric(diag_1), 800, 999) ~ "Injury/Poisoning",
    TRUE ~ "Other"
  ))

# Secondary Diagnosis
df <- df %>%
  mutate(secondary_diagnosis = case_when(
    substr(diag_2, 1, 3) == "250" ~ "Diabetes",
    #between(as.numeric(diag_2), 001, 139) ~ "Infectious and Parasitic Diseases",
    between(as.numeric(diag_2), 140, 239) ~ "Neoplasms",
    between(as.numeric(diag_2), 320, 459) ~ "Circulatory",
    between(as.numeric(diag_2), 460, 519) ~ "Respiratory",
    between(as.numeric(diag_2), 520, 579) ~ "Digestive",
    between(as.numeric(diag_2), 580, 629) ~ "Genitourinary",
    #between(as.numeric(diag_2), 680, 709) ~ "Skin/Subcutaneous Tissue",
    between(as.numeric(diag_2), 710, 739) ~ "Musculoskeletal",
    between(as.numeric(diag_2), 760, 779) ~ "Perinatal",
    between(as.numeric(diag_2), 800, 999) ~ "Injury/Poisoning",
    TRUE ~ "Other"
  ))

# Secondary Diagnosis
df <- df %>%
  mutate(secondary_diagnosis2 = case_when(
    substr(diag_3, 1, 3) == "250" ~ "Diabetes",
    #between(as.numeric(diag_3), 001, 139) ~ "Infectious and Parasitic Diseases",
    between(as.numeric(diag_3), 140, 239) ~ "Neoplasms",
    between(as.numeric(diag_3), 320, 459) ~ "Circulatory",
    between(as.numeric(diag_3), 460, 519) ~ "Respiratory",
    between(as.numeric(diag_3), 520, 579) ~ "Digestive",
    between(as.numeric(diag_3), 580, 629) ~ "Genitourinary",
    #between(as.numeric(diag_3), 680, 709) ~ "Skin/Subcutaneous Tissue",
    between(as.numeric(diag_3), 710, 739) ~ "Musculoskeletal",
    between(as.numeric(diag_3), 760, 779) ~ "Perinatal",
    between(as.numeric(diag_3), 800, 999) ~ "Injury/Poisoning",
    TRUE ~ "Other"
  ))
df$primary_diagnosis <- as.factor(df$primary_diagnosis)
df$secondary_diagnosis <- as.factor(df$secondary_diagnosis)
df$secondary_diagnosis2 <- as.factor(df$secondary_diagnosis2)
```

* Show counts of diseases for the primary and secondary diagnosis:

```{r}
table(df$primary_diagnosis)
# table(df$secondary_diagnosis)
# table(df$secondary_diagnosis2)
```

* All levels w/ less than 2500 instances are combined into 'Other'
* Most common primary/secondary diagnosis were circulatory. 
* 7185 patients had diabetes as their primary diagnosis

* diag_1, diag_2, and diag_3 are turned to new variables. They are not needed anymore and will be removed.

```{r}
df <- subset(df, select = -c(diag_1, diag_2,diag_3))
dim(df)
```

#### <br><b>1.7 Convert categorical variables into factors</b><br><br>

* All feature that are recorded as chr type are categorical variables 
* admission_type_id, discharge_disposition_id, admission_source_id are recorded as int, but should be taken as categorical variables.

```{r}
#Convert categorical variables into factors (according to Table 1 https://www.hindawi.com/journals/bmri/2014/781670/tab1/)
df[sapply(df, is.character)] <- lapply(df[sapply(df, is.character)], 
                                       as.factor)

df$admission_type_id <- as.factor(df$admission_type_id)
df$discharge_disposition_id <- as.factor(df$discharge_disposition_id)
df$admission_source_id <- as.factor(df$admission_source_id)

```

* All categorical variables have been converted into factors

#### <br><b>1.8 Classify features</b><br><br>

* 27 variables are left for further analysis.
* Subset numerical and categorical features

```{r message=FALSE, warning=FALSE}
# Subset numerical and categorical features into new data frames
library(dplyr)
df_num <- df %>%  select_if(is.integer)
df_cat <- df %>% select_if(is.factor)

# Show numerical variables
cat("These are the numerical features:\n", colnames(df_num), "\n\n")

# Show categorical variables
cat("These are the categorical features:\n", colnames(df_cat), "\n\n")
```

#### <br><b>1.9 Explore and revalue the response variable</b><br><br>

* We use "readmitted" as our response variable 

```{r}
# Show readmission counts
table(df$readmitted)
```

We will combine all patients who were admitted into one level. Thus, our response variable will be binary: 

* YES if the patient was readmitted at any time
* NO if the patient was not readmitted 

```{r message=FALSE, warning=FALSE}
library(plyr)

revalue(df$readmitted, c("<30" = "YES")) -> df$readmitted
revalue(df$readmitted, c(">30" = "YES")) -> df$readmitted

# Show readmission counts using binary levels
table(df$readmitted)

# Show readmission ratio using binary levels
round(table(df$readmitted) / length(df$readmitted),2)
```

* The response variable now contains 47% of "YES" and 53% of "NO".


### <br>2. Exploratory Data Analysis<br>

#### <br><b>2.1 Statistics of the numerical variables</b><br><br>

```{r}
summary(df_num)
```

#### <br><b>2.2 Correlations of the numerical variables</b><br><br>

* Find Correlations of numerical features

```{r message=FALSE, warning=FALSE, out.width="500"}

library(corrplot)
correlations <- round(cor(df_num), 2)
correlations

# Correlation plot
corrplot(correlations, order = "hclust")
```

Observing the correlation table and heatmap, there is modest correlations between:

* num_lab_procedures and time_in_hospital (0.33)
* num_medications and and time_in_hospital (0.46)
* num_lab_procedures and num_medications (0.38)

Other than that, there is no much correlations between other variables.


#### <br><b>2.3 Distribution of the numerical variables</b><br><br>

```{r out.width="1000", out.height="1000", message=FALSE, warning=FALSE}
library(Hmisc)
hist.data.frame(df_num)
```

* number_medications and number_lab_procedures have a distribution that is close to normal distribution. The distribution for other variables are all skewed.


#### <br><b>2.4 Explore the relationship between the numerical predictors and the predicted variable</b><br><br>

```{r out.width="1000", out.height="700"}

par(mfrow = c(2,4))

boxplot(df$time_in_hospital ~ df$readmitted, col="blue")
boxplot(df$num_lab_procedures ~ df$readmitted, col="orange")
boxplot(df$number_diagnoses ~ df$readmitted, col="blue")

boxplot(df$number_emergency ~ df$readmitted, col="orange")
boxplot(df$num_medications ~ df$readmitted, col="blue")
boxplot(df$number_inpatient ~ df$readmitted, col="orange")
boxplot(df$number_outpatient ~ df$readmitted, col="blue")

```
Patients who are readmitted, in general:

* Spend more time in the hospital
* Have more lab procedures done


#### <br><b>2.5 Distribution of the categorical variables</b><br><br>

```{r out.width="1100", out.height="900"}
par(mfrow = c(3,3))

barplot(table(df$age), main="Distribution of Age", col="blue", las=2)
barplot(table(df$race), main="Distribution of Race", col="orange", las=2)
barplot(table(df$gender), main="Distribution of Gender", col="blue", las=2)

barplot(table(df$admission_type_id), main="Distribution of admission_type_id", 
        col="orange", las=2)
barplot(table(df$discharge_disposition_id), main="Distribution of discharge_disposition_id", 
        col="blue", las=2)
barplot(table(df$admission_source_id), main="Distribution of admission_source_id", 
        col="orange",las=2)


barplot(table(df$primary_diagnosis), main="Distribution of primary_diagnosis", 
        col="blue", las=2)
barplot(table(df$secondary_diagnosis), main="Distribution of secondary_diagnosis", 
        col="orange", las=2)
barplot(table(df$secondary_diagnosis2), main="Distribution of secondary_diagnosis2", 
        col="blue", las=2)
```



```{r out.width="1100", out.height="900"}
par(mfrow = c(3,3))

barplot(table(df$diabetesMed), main="Distribution of diabetesMed", col="blue", las=2)
barplot(table(df$A1Cresult), main="Distribution of A1Cresult", col="orange", las=2)
barplot(table(df$metformin), main="Distribution of metformin", col="blue", las=2)


barplot(table(df$glipizide), main="Distribution of glipizide", col="orange", las=2)
barplot(table(df$glyburide), main="Distribution of glyburide", col="blue", las=2)
barplot(table(df$pioglitazone), main="Distribution of pioglitazone", col="orange", las=2)


barplot(table(df$rosiglitazone), main="Distribution of rosiglitazone", col="blue", las=2)
barplot(table(df$insulin), main="Distribution of insulin", col="orange", las=2)
barplot(table(df$change), main="Distribution of change", col="blue", las=2)

```


#### <br><b>2.6 Explore the relationship between the categorical predictors and the predicted variable</b><br><br>

```{r}
# Frequency counts
ct_race <- table(df$readmitted, df$race)
ct_gender <- table(df$readmitted, df$gender)
ct_age <- table(df$readmitted, df$age)
ct_admissionType <- table(df$readmitted, df$admission_type_id)
ct_dischargeID <- table(df$readmitted, df$discharge_disposition_id)
ct_admissionSource <- table(df$readmitted, df$admission_source_id)
ct_A1C <- table(df$readmitted, df$A1Cresult)
ct_metf <- table(df$readmitted, df$metformin)
ct_glipz <- table(df$readmitted, df$glipizide)
ct_glyb <- table(df$readmitted, df$glyburide)
ct_piog <- table(df$readmitted, df$pioglitazone)
ct_rosig <- table(df$readmitted, df$rosiglitazone)
ct_insulin <- table(df$readmitted, df$insulin)
ct_change <- table(df$readmitted, df$change)
ct_diabMeds <- table(df$readmitted, df$diabetesMed)

# Create contingency tables showing proportions 
ct_race1 <- prop.table(table(df$readmitted, df$race),2)
ct_gender1 <- prop.table(table(df$readmitted, df$gender),2)
ct_age1 <- prop.table(table(df$readmitted, df$age),2)
ct_admissionType1 <- prop.table(table(df$readmitted, df$admission_type_id),2)
ct_dischargeID1 <- prop.table(table(df$readmitted, df$discharge_disposition_id),2)
ct_admissionSource1 <- prop.table(table(df$readmitted, df$admission_source_id),2)
ct_A1C1 <- prop.table(table(df$readmitted, df$A1Cresult),2)
ct_metf1 <- prop.table(table(df$readmitted, df$metformin),2)
ct_glipz1 <- prop.table(table(df$readmitted, df$glipizide),2)
ct_glyb1 <- prop.table(table(df$readmitted, df$glyburide),2)
ct_piog1 <- prop.table(table(df$readmitted, df$pioglitazone),2)
ct_rosig1 <- prop.table(table(df$readmitted, df$rosiglitazone),2)
ct_insulin1 <- prop.table(table(df$readmitted, df$insulin),2)
ct_change1 <- prop.table(table(df$readmitted, df$change),2)
ct_diabMeds1 <- prop.table(table(df$readmitted, df$diabetesMed),2)
```


```{r out.width="1200", out.height="1000"}

## Create barplots ##
# Patient Demographics (Legend removed for individual graphs where bars would be blocked)
par(mfrow=c(3,3))
barplot(ct_age1,main="Age Group", ylab="Readmitted",col=c("blue", "orange"))
barplot(ct_race1,main="Race", ylab="Readmitted",col=c("blue", "orange"),las=2)
barplot(ct_gender1,main="Gender", ylab="Readmitted",col=c("blue", "orange"),
        legend=rownames(ct_gender1))

barplot(ct_dischargeID1,main="Discharge ID", ylab="Readmitted",col=c("blue", "orange"))
barplot(ct_admissionSource1,main="Admission Source", 
        ylab="Readmitted",col=c("blue", "orange"),las=2)
barplot(ct_admissionType1,main="Admission Type", ylab="Readmitted",col=c("blue", "orange"))


barplot(ct_A1C1,main="A1C Result", ylab="Readmitted",col=c("blue", "orange"),legend=rownames(ct_A1C))
barplot(ct_change1,main="Medications Change", ylab="Readmitted",col=c("blue", "orange"))
barplot(ct_diabMeds1,main="Diabetes Medication",ylab="Readmitted",col=c("blue", "orange"))

```

* Race: Asians were disproportionately less likely to be readmitted. 
* Discharge ID: Discharge IDs 11, 19, and 20 were disproportionately less likely to be readmitted. Patients with discharge IDs 10,12,15 were disproportionately more likely to be readmitted. 
* Admission sources 10, 11, 13, 14, 22, and 25 were less likely to be readmitted (0% readmission rates)
* Admission type 7 proportionately less likely to be readmitted
* No significant differences for the change in/prescription of drugs were observed. 


### <br>3. Data Preprocessing

#### <br><b>3.1 Select features</b><br><br>

* From EDA, we observe that variables number_emergency, number_outpatient, number_inpatient, A1Cresult, glyburide, pioglitazone, pioglitazone, rosiglitazone, glipizide show low variance or are extremely skewed. They will be excluded from modeling.

```{r}
df <- subset(df, select = -c(number_emergency, number_outpatient, number_inpatient, A1Cresult, glyburide, pioglitazone, pioglitazone, rosiglitazone, glipizide))

dim(df)
```

* After this step, 19 variables are left for modeling.

#### <br><b>3.2 Remove outliers</b><br><br>

* From visualizations in the EDA section, we observe some NUmerical variables including num_lab_procedures, number_diagnoses, and num_medications have outliers. We use 1.5 IQR to remove them.

```{r}
# Remove outlier from column 'num_lab_procedures'
quartiles <- quantile(df$num_lab_procedures, probs=c(.25, .75), na.rm = FALSE)
IQR <- IQR(df$num_lab_procedures)
Lower <- quartiles[1] - 1.5*IQR
Upper <- quartiles[2] + 1.5*IQR

count_before <-dim(df)[1]
df<- subset(df, df$num_lab_procedures > Lower & df$num_lab_procedures < Upper)
count_after <-dim(df)[1]
cat(count_before-count_after, " outliers in num_lab_procedures have been removed. \n")


# Remove outliers from column 'number_diagnoses'
quartiles <- quantile(df$number_diagnoses, probs=c(.25, .75), na.rm = FALSE)
IQR <- IQR(df$number_diagnoses)
Lower <- quartiles[1] - 1.5*IQR
Upper <- quartiles[2] + 1.5*IQR

count_before <-dim(df)[1]
df <- subset(df, df$number_diagnoses > Lower & df$number_diagnoses < Upper)
count_after <-dim(df)[1]
cat(count_before-count_after, " outliers in number_diagnoses have been removed. \n")


# Remove outliers from column 'num_medications'
quartiles <- quantile(df$num_medications, probs=c(.25, .75), na.rm = FALSE)
IQR <- IQR(df$num_medications)
Lower <- quartiles[1] - 2*IQR
Upper <- quartiles[2] + 2*IQR

count_before <-dim(df)[1]
df <- subset(df, df$num_medications > Lower & df$num_medications < Upper)
count_after <-dim(df)[1]
cat(count_before-count_after, " outliers in num_medications have been removed. \n")

```

#### <br><b>3.3 Partition data into training and test datasets using a 70% ratio</b><br><br>

* readmitted is the predicted variable. It will be separated from the predictors.

```{r}
#Separate X and Y using readmitted as predicted variable
dfX <- subset(df, select = -c(readmitted))
dfY <- subset(df, select = c(readmitted))

set.seed(123)
# Partition the dataset
trainRows <- createDataPartition(dfY$readmitted, p = .70, list = FALSE) 

trainX <- dfX[trainRows,]
testX <-  dfX[-trainRows,]

trainY <- dfY[trainRows,]
testY <- dfY[-trainRows,]

dim(trainX)
dim(testX)

```

* After the partition, there are 59529 instances in the training set and 25512 instances in the test set.


#### <br><b>3.4 Trandform and standardize features: center and scale</b><br><br>

```{r}
# center and scale the training set 
train_tran <- preProcess(trainX, method=c("center", "scale"))
trainX <- predict(train_tran, trainX)

# center and scale the test set
testX <- predict(train_tran, testX)

```


#### <br><b>3.5 Convert categorical variables to dummy variables</b><br><br>

* Backup the training and testing sets before adding dummy variables for training different models

```{r message=FALSE, warning=FALSE}
library(fastDummies)

# Some models do not require dummy variable, like the trees
# Will use the sets without dummies for models
trainX_noDummy <- trainX
testX_noDummy <- testX

colnames(trainX_noDummy)
```

* Categorical variables will now be converted into n-1 dummy variables. 

```{r}
# Add dummy variables
trainX <- dummy_cols(trainX, 
                     select_columns=c("race", "gender","age",
                                      "admission_type_id", "discharge_disposition_id","metformin",
                                      "insulin", "change","diabetesMed",
                                      "admission_source_id","primary_diagnosis",
                                      "secondary_diagnosis", "secondary_diagnosis2"), 
                     remove_first_dummy=TRUE,
                     remove_selected_columns=TRUE)

testX <- dummy_cols(testX, 
                     select_columns=c("race", "gender","age",
                                      "admission_type_id", "discharge_disposition_id","metformin",
                                      "insulin", "change","diabetesMed",
                                      "admission_source_id","primary_diagnosis",
                                      "secondary_diagnosis", "secondary_diagnosis2"), 
                     remove_first_dummy=TRUE,
                     remove_selected_columns=TRUE)

```


#### <br><b>3.6 PCA Analysis</b><br><br>

* This is a high-dimensional dataset, we want to see if PCA can be applied to reduce feature dimension.

```{r}
# Find the PCA from the training set
pca <- prcomp(trainX)
pca_var <- pca$sdev^2

# Find the percentage of each PCA component
pca_percents <- pca_var / sum(pca_var)
cat("Percentage of the first 15 PCAs: ", pca_percents[1:15], "\n")

# The total percentage of the first 35 PCAs
cat("The total percentage of the first 35 PCAs: ", sum(pca_percents[1:35]), "\n")
```

* We see there are no dominant PCA components. The first 35 components represent 94% of the features. 


### <br>4. Modeling

#### <br><b>4.0 Define  functions</b><br><br>

First we will define a function to calculate the metrics of a trained model

* The function will take one parameters: a trained model
* The function will calculate these metrics including accuracy, sensitivity, specificity, precision
* It will return a vector containing those metrics

```{r}
# Define a function to calculate the metrics of a trained model
get_training_metrics <- function(model, roc) {
  
  # Total number of training objects
  total <- dim(trainX)[1]
    
 # Construct model's confusion matrix
  cm <- confusionMatrix(model, norm = "none")

  # Calculate metrics
  accuracy <- round((cm$table[1,1] + cm$table[2,2]) / total, 3)
  sensitivity <- round(cm$table[1,1] / (cm$table[1,1] + cm$table[2,1]), 3)
  specificity <- round(cm$table[2,2] / (cm$table[2,2] + cm$table[1,2]), 3) 
  precision <- round(cm$table[1,1] / (cm$table[1,1] + cm$table[1,2]), 3) 
  recall <- sensitivity
  F1 <- round(2 * recall * precision / (recall + precision ), 3)
  
  # Return a vector of metrics
  c(accuracy, sensitivity, specificity, precision, recall, F1, round(roc,3))
}

```

Next we will define a function to calculate the metrics of prediction result on test dataset

* The function will take one parameters: predicted results of the test set
* The function will calculate these metrics including ROC, accuracy, sensitivity, specificity, precision
* It will return a vector containing those metrics

```{r}
# Define a function to calculate the metrics of a trained model
get_test_metrics <- function(test_results) {
  total <- dim(testX)[1]
  
  # Construct prediction's confusion matrix  
  cm <- confusionMatrix(test_results, testY, positive = "YES")

  # Calculate metrics
  accuracy <- round((cm$table[1,1] + cm$table[2,2]) / total, 3)
  sensitivity <- round(cm$table[1,1] / (cm$table[1,1] + cm$table[2,1]), 3)
  specificity <- round(cm$table[2,2] / (cm$table[2,2] + cm$table[1,2]), 3) 
  precision <- round(cm$table[1,1] / (cm$table[1,1] + cm$table[1,2]), 3) 
  recall <- sensitivity
  F1 <- round(2 * recall * precision / (recall + precision ), 3)
  # Return a vector of metrics
  c(accuracy, sensitivity, specificity, precision, recall, F1)
}

```


#### <br><b>4.1 Logistic Regression Model</b><br><br>

* First we will define a control variable that will be used to train all the models.
* We will use a 10 fold cross-validation method to optimize the training process.
* For each model, we will first train the model using training dataset, then use test dataset to test the model.
* We will generate a data frame to hold the training and testing performance metrics.

```{r warning=FALSE}

# Define train control
ctrl <- trainControl(method = "cv", summaryFunction = twoClassSummary, 
                     classProbs = TRUE, savePredictions = "final")

# Logistic Regression 
set.seed(123)
lrFit <- train(x = trainX, y = trainY,
               method = "glm", metric = "ROC", trControl = ctrl)

# Calculate training/resampling performance metrics
metrics_tr <- data.frame(Metric.Train = c("Accuracy", "Sensitivity", "Specificity", "Precision", "Recall", "F-Measure", "ROC")) 

metrics_tr$LR <- get_training_metrics(lrFit, lrFit$results$ROC)

#metrics_tr

# Predict on test data
lrTestResults <- predict(lrFit, testX)

# Calculate test performance metrics
metrics_test <- data.frame(Metric.Test = c("Accuracy", "Sensitivity", "Specificity", "Precision", "Recall", "F-Measure")) 
metrics_test$LR <- get_test_metrics(lrTestResults)

# Importance of the predictors
lrImp <- varImp(lrFit, scale = FALSE)

# Display model's performance
metrics_tr[, c("Metric.Train", "LR")]
metrics_test[, c("Metric.Test", "LR")]

```

* The training (cross validation) performance and testing performance for logistic regression model is displayed in the above metrics. 


#### <br><b>4.2 Penalized Logistic Regression Model</b><br><br>

For penalized logistic regression model, we use the following perimeters to build the tuning grid:

* alpha values: 0, 0,0.1,0.2,0.4
* regularization perimeter lambda takes 5 values evenly between 0.01 and 0.1
* length: 5

```{r out.width="450"}
# Penalized Logistic Regression
set.seed(123)
glmnGrid <- expand.grid(alpha=c(0,0.1,0.2,0.4),
                        lambda=seq(.01, .1, length=5))
glmnFit <- train(x = trainX, y = trainY, 
                 method="glmnet", tuneGrid=glmnGrid, 
                 metric="ROC", trControl=ctrl)
#glmnFit

# Calculate training/resampling performance metrics
metrics_tr$GLMN <- get_training_metrics(glmnFit, glmnFit$results$ROC[1])
#metrics_tr

# Predict on test data
glmnTestResults <- predict(glmnFit, testX)

# Calculate test performance metrics
metrics_test$GLMN <- get_test_metrics(glmnTestResults)

# Importance of the predictors
glmnImp <- varImp(glmnFit, scale = FALSE)


# Plot the tuning results
plot(glmnFit)

# Display model's performance
metrics_tr[, c("Metric.Train", "GLMN")]
metrics_test[, c("Metric.Test", "GLMN")]
```

* The cross validation result shows the optimal model comes when alpha=0, lambda=0.01

* The training (cross validation) performance and testing performance is displayed in the above metrics. 


#### <br><b>4.3 Nearest Shrunken Centroids Model</b><br><br>

For Nearest Shrunken Centroids model, we use the following perimeter to build the tuning grid:

* threshold takes 20 values evenly spaced between 0 and 15


```{r out.width="450", out.height="400"}

# Nearest Shrunken Centroids
set.seed(123)
nscFit <- train(x=trainX,y=trainY,
                method="pam", tuneGrid=data.frame(threshold=seq(0,15, length=20)),
                metric="ROC", trControl=ctrl)
#nscFit

# Calculate training/resampling performance metrics
metrics_tr$NSC <- get_training_metrics(nscFit, nscFit$results$ROC[1])

# Predict on test data
nscTestResults <- predict(nscFit, testX)

# Calculate test performance metrics
metrics_test$NSC <- get_test_metrics(nscTestResults)

# Importance of the predictors
nscImp <- varImp(nscFit, scale = FALSE)

# Plot the tuning result
plot(nscFit)

# Display model's performance
metrics_tr[, c("Metric.Train", "NSC")]
metrics_test[, c("Metric.Test", "NSC")]
```
* The cross validation result shows the optimal model comes when shrinkage threshhold is 0.

* The training (cross validation) performance and testing performance is displayed in the above metrics. 


#### <br><b>4.4 Boosted Trees </b><br><br>

For the boosted trees model, we use the following perimeter to build the tuning grid:

* interaction depth: 5, 7
* number of trees: 500
* shrinkage: 0.01, 0.1

```{r out.width="450"}

gbmGrid <- expand.grid(interaction.depth = c(5, 7),
                       n.trees = 500, 
                       shrinkage = c(.01, .1),
                       n.minobsinnode = 5)

set.seed(123)

gbmFit <- train(x = trainX_noDummy, y = trainY,
                method = "gbm", tuneGrid = gbmGrid,
                verbose = FALSE, metric = "ROC", trControl = ctrl)

# Calculate training/resampling performance metrics
metrics_tr$GBM <- get_training_metrics(gbmFit, gbmFit$results$ROC[3])
#metrics_tr

# Predict on test data
gbmTestResults <- predict(gbmFit, testX_noDummy)

# Calculate test performance metrics
metrics_test$GBM <- get_test_metrics(gbmTestResults)

#The tunning result
#gbmFit
plot(gbmFit)

# Display model's performance
metrics_tr[, c("Metric.Train", "GBM")]
metrics_test[, c("Metric.Test", "GBM")]
```

* The cross validation result shows the optimal model comes when shrinkage is 0.1 and depth is 5..

* The training (cross validation) performance and testing performance is displayed in the above metrics. 


#### <br><b>4.5 Bagged Tree</b><br><br>

For bagged tree model, we use the following perimeter to train:

* number of bag(nbagg) = 30

```{r}
set.seed(123)

trbagFit <- train(x = trainX_noDummy, y = trainY,
                method = "treebag",
                nbagg = 30,
                metric = "ROC",
                trControl = ctrl)

#trbagFit

# Calculate training/resampling performance metrics
metrics_tr$TRBAG <- get_training_metrics(trbagFit, trbagFit$results$ROC)

# Predict on test data
trbagTestResults <- predict(trbagFit, testX_noDummy)

# Calculate test performance metrics
metrics_test$TRBAG <- get_test_metrics(trbagTestResults)

# Display model's performance
metrics_tr[, c("Metric.Train", "TRBAG")]
metrics_test[, c("Metric.Test", "TRBAG")]

```

* The training (cross validation) performance and testing performance is displayed in the above metrics. 


#### <br><b>4.6 Random Forest Tree</b><br><br>

For random forest tree model, we use the following perimeter to build the tuning grid:

* number of randomly selected predictors: 1, 3, 5, 7
* number of trees: 100

```{r out.width="450"}
mtryValues <- seq(1,8,2)

set.seed(123)
rfFit <- train(x = trainX_noDummy, y = trainY,
                method = "rf",
                ntree = 100,
                tuneGrid = data.frame(mtry = mtryValues),              
                metric = "ROC",
                trControl = ctrl)


# Calculate training/resampling performance metrics
metrics_tr$RF <- get_training_metrics(rfFit, rfFit$results$ROC[2])

# Predict on test data
rfTestResults <- predict(rfFit, testX_noDummy)

# Calculate test performance metrics
metrics_test$RF <- get_test_metrics(rfTestResults)

# Importance of the predictors
rfImp <- varImp(rfFit, scale = FALSE)

# Plot tuning result
#rfFit
plot(rfFit)

# Display model's performance
metrics_tr[, c("Metric.Train", "RF")]
metrics_test[, c("Metric.Test", "RF")]

```

* The cross validation result shows the optimal model comes when the number of randomly selected predictors is 3

* The training (cross validation) performance and testing performance is displayed in the above metrics. 


#### <br><b>4.7 K-Nearest Neighbor (KNN) Model</b><br><br>

For KNN model, we use the following perimeter to build the tuning grid:

* tuneLength = 3 
* number of neighbors: 5, 7, 9

```{r out.width="450"}
set.seed(123)

knnFit <- train(x = trainX, y = trainY,
                 method = "knn", 
                 tuneLength = 3,
                 metric = "ROC", trControl = ctrl)


# Calculate training/resampling performance metrics
metrics_tr$KNN <- get_training_metrics(knnFit, knnFit$results$ROC[3])

# Predict on test data
knnTestResults <- predict(knnFit, testX)

# Calculate test performance metrics
metrics_test$KNN <- get_test_metrics(knnTestResults)

# Importance of the predictors
knnImp <- varImp(knnFit, scale = FALSE)

# Plot the tuning results
#knnFit
plot(knnFit)

# Display model's performance
metrics_tr[, c("Metric.Train", "KNN")]
metrics_test[, c("Metric.Test", "KNN")]

```

* The cross validation result shows the optimal model comes when K is 9.

* The training (cross validation) performance and testing performance is displayed in the above metrics. 


### <br>5. Model Evaluation and Conclusion

#### <br><b>5.1 Baseline Model </b><br><br>

* For this data analysis, a model's ability to predict the positive (readmitted-Yes) accurately is the
most important metric. Therefore, we choose All Positive Model as the base model. 

```{r}

round(table(trainY) / length(trainY), 3)

```

* From the above table, we see that when we assign all predictions as positive, the accuracy of this base model is 0.476


#### <br><b>5.2 Calculate AUC (Area Under ROC Curve) </b><br><br>

For each model, based on the trained models' result, we first generate ROC, then calculate the area under ROC curve (AUC)

```{r message=FALSE, warning=FALSE}
#ROC Curve
library(pROC)
lrROC <- roc(response=trainY,predictor=lrFit$pred$YES,levels=rev(levels(lrFit$pred$obs)))
glmnROC <- roc(response=trainY,predictor=glmnFit$pred$YES,levels=rev(levels(glmnFit$pred$obs)))
nscROC <- roc(response=trainY,predictor=nscFit$pred$YES,levels=rev(levels(nscFit$pred$obs)))
gbmROC <- roc(response=trainY,predictor=gbmFit$pred$YES,levels=rev(levels(gbmFit$pred$obs)))
rfROC <- roc(response=trainY,predictor=rfFit$pred$YES,levels=rev(levels(rfFit$pred$obs)))
trbagROC <- roc(response=trainY,predictor=trbagFit$pred$YES,levels=rev(levels(trbagFit$pred$obs)))
knnROC <- roc(response=trainY,predictor=knnFit$pred$YES,levels=rev(levels(knnFit$pred$obs)))

lrAUC <-round(auc(lrROC), 3)
glmnAUC <-round(auc(glmnROC), 3)
nscAUC <-round(auc(nscROC), 3)
gbmAUC <-round(auc(gbmROC), 3)
rfAUC <-round(auc(rfROC), 3)
trbagAUC <-round(auc(trbagROC), 3)
knnAUC <-round(auc(knnROC), 3)

# Get Area under the ROC
metrics_tr <- rbind(metrics_tr, c("AUC", lrAUC, glmnAUC, nscAUC, 
                                  gbmAUC, rfAUC, trbagAUC, knnAUC))

```

* The AUC values are added to the training model's performance metrics data frame.


#### <br><b>5.3 Compare models' performance metrics </b><br><br>

* Compare Models' training (cross-validation) performance

```{r}
metrics_tr
```

* Compare Models' test performance

```{r}
metrics_test
```

#### <br><b>5.4 Plot Roc Curves </b><br><br>

* We plot the ROC curves for all trainings models in one graph. The comparison can be seen with different marking colors.

```{r out.width="450"}
plot(lrROC, type="s", col='yellow', legacy.axes=TRUE)
plot(glmnROC, type="s", add=TRUE, col='green', legacy.axes=TRUE)
plot(nscROC, type="s", add=TRUE, col='black', legacy.axes=TRUE)
plot(gbmROC, type="s", add=TRUE, col='orange',legacy.axes=TRUE)
plot(rfROC, type="s", add=TRUE, col='purple', legacy.axes=TRUE)
plot(trbagROC, type="s", add=TRUE, col='blue',legacy.axes=TRUE)
plot(knnROC, type="s", add=TRUE, col='red', legacy.axes=TRUE)

legend("bottomright", legend=c("LR", "GLMNET", "NSC", "GBM", "RF", "TRBAG", "KNN"),
       col=c("yellow","green", "black",'orange', 'purple','blue', 'red'), lwd=2)


title(main="Compare ROC Curves")
```

#### <br><b>5.5 Check Feature's Importance </b><br><br>

* We displayed the first 15 important features from logistic regression model, random forest model, and KNN model respectively.

```{r out.width="450"}
plot(lrImp, top = 15)
plot(rfImp, top = 15)
plot(knnImp, top = 15)
```

* Though each model displays the important features in different order, some features appear commonly in the top list. 

* These important features include: number_diagnoses, num_procedures, num_lab_procedures, num_medication, time_in_hospital. etc.
